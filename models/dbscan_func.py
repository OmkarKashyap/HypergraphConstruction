# -*- coding: utf-8 -*-
"""DBSCAN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Kl6HQ2Rom7E07btPJZmdjAEQDN3QVwfL
"""

from sklearn.cluster import DBSCAN
from transformers import BertTokenizer, BertModel
import torch
import numpy as np
import nltk
from nltk.corpus import stopwords

def get_embeddings(sentence, model, tokenizer):
    # tokens = tokenizer(sentence, return_tensors="pt")
    # with torch.no_grad():
    #     output = model(**tokens)
    # embeddings = output.last_hidden_state.mean(dim=1).squeeze().numpy()
    nltk.download('stopwords')
    stop_words = stopwords.words('english')
    sentence = sentence.lower()
    sentence = ''.join([c for c in sentence if c.isalnum() or c.isspace()])
    words = [word for word in sentence.split() if word not in stop_words]
    cleaned_sentence = ' '.join(words)
    tokens = tokenizer.encode(cleaned_sentence, return_tensors='pt')
    with torch.no_grad():
      embeddings = model(tokens).last_hidden_state
    word_dimensions = []
    entities = tokenizer.convert_ids_to_tokens(tokens[0])
    for token_idx in range(embeddings.shape[1]):
        word_embedding = embeddings[:, token_idx, :]
        word_dimensions.append(word_embedding)
    entity_embeddings = {entity: embedding for entity, embedding in zip(entities, word_dimensions)}
    return embeddings, tokens

def hypergraph_generation(sentence, model, tokenizer, epsilon=0.01, min_samples=8):
    clusters = []
    entity_clusters = []
    hyperedges = []
    answer_embeddings, answer_tokens = get_embeddings(sentence, model, tokenizer)
    go = True
    num_dimensions = answer_embeddings.shape[2]

    for dim_to_cluster in range(num_dimensions):
      current_dimension_embeddings = answer_embeddings.squeeze(0)[:, dim_to_cluster].reshape(-1, 1)
      db = DBSCAN(eps=epsilon, min_samples=min_samples)
      labels = db.fit_predict(current_dimension_embeddings)
      unique_labels = set(labels)
      unique_labels.remove(-1)
      current_clusters = [answer_tokens[0][labels == label].reshape(-1) for label in unique_labels]
      tuple_clusters = [tuple(tensor.numpy()) for tensor in current_clusters]
      for i in current_clusters:
        temp_cluster = tokenizer.convert_ids_to_tokens(i)
        temp_cluster = tuple(temp_cluster)
        entity_clusters.append(temp_cluster)
      if go:
        go = False
      hyperedges.extend(entity_clusters)
      clusters.extend(current_clusters)
      # print(f"{dim_to_cluster} : {entity_clusters}")
    hyperedges = list(set(hyperedges))

    return clusters, hyperedges

def calculate_euclidean_distance(centroid1, centroid2):
    distance = torch.sqrt(torch.sum((centroid1 - centroid2) ** 2))
    return distance.item()  # Convert the tensor to a Python scalar

def calculate_similarity_matrix(cluster_embeddings):
    num_clusters = len(cluster_embeddings)
    similarity_matrix = np.zeros((num_clusters, num_clusters))

    for i in range(num_clusters):
        for j in range(num_clusters):
            centroid1 = torch.mean(cluster_embeddings[i], dim=0)
            centroid2 = torch.mean(cluster_embeddings[j], dim=0)
            similarity = calculate_euclidean_distance(centroid1, centroid2)
            similarity_matrix[i, j] = similarity

    return similarity_matrix

def form_similarity_matrix(clusters, model):
    cluster_embeddings = []
    for cluster in clusters:
        tokens = torch.tensor(cluster).unsqueeze(0)
        with torch.no_grad():
            embeddings = model(tokens).last_hidden_state.mean(dim=1)
        cluster_embeddings.append(embeddings)
    similarity_matrix = calculate_similarity_matrix(cluster_embeddings)
    return similarity_matrix


sentence = '''
Lionel Andr√©s "Leo" Messi[note 1]  born 24 June 1987) is an Argentine professional footballer who plays as a forward for and captains both Major League Soccer club Inter Miami and the Argentina national team. Widely regarded as one of the greatest players of all time, Messi has won a record eight Ballon d'Or awards, a record six European Golden Shoes, and was named the world's best player for a record eight times by FIFA.[note 2] Until leaving the club in 2021, he had spent his entire professional career with Barcelona, where he won a club-record 34 trophies, including ten La Liga titles, seven Copa del Rey titles, and the UEFA Champions League four times
'''
model_name = "bert-base-uncased"
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

result_clusters, hyperedges = hypergraph_generation(sentence, model, tokenizer)
for i in hyperedges:
  print(i)
similarity_matrix = form_similarity_matrix(result_clusters, model)
# print("Similarity Matrix:")
# print(similarity_matrix)




